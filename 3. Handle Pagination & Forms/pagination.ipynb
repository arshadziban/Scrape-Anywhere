{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f759a1d0",
   "metadata": {},
   "source": [
    "# Handling Pagination in Web Scraping\n",
    "\n",
    "Pagination splits website content across multiple pages to improve loading times and user experience. When scraping data that spans several pages, we need strategies to handle this pagination effectively.\n",
    "\n",
    "## Common Pagination Patterns\n",
    "\n",
    "### URL-Based Pagination\n",
    "Many websites use predictable URL patterns:\n",
    "- `example.com/page/1`\n",
    "- `example.com/page/2` \n",
    "- `example.com/page/3`\n",
    "\n",
    "### Button-Based Pagination\n",
    "Sites may use \"Next\" or numbered buttons for navigation:\n",
    "- Through HTML elements (`<a>`, `<button>`)\n",
    "- Via JavaScript events\n",
    "- Using AJAX requests\n",
    "\n",
    "## Implementation Approaches\n",
    "\n",
    "1. **URL Pattern Iteration**\n",
    "    - Construct URLs systematically\n",
    "    - Process each page sequentially\n",
    "\n",
    "2. **Dynamic Navigation**\n",
    "    - Follow \"Next\" page links\n",
    "    - Handle JavaScript-based pagination\n",
    "\n",
    "3. **API Integration**\n",
    "    - Use site's API if available\n",
    "    - Often more reliable than HTML scraping\n",
    "\n",
    "Remember to implement delays between requests and respect the site's robots.txt guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe148dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes with authors saved to quotes.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"quotes.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    \n",
    "    for page in range(1, 4):  # Pages 1, 2, 3\n",
    "        url = f\"https://quotes.toscrape.com/page/{page}/\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        quote_blocks = soup.find_all(\"div\", class_=\"quote\")\n",
    "        for quote_block in quote_blocks:\n",
    "            quote_text = quote_block.find(\"span\", class_=\"text\").text\n",
    "            author = quote_block.find(\"small\", class_=\"author\").text\n",
    "            file.write(f\"{quote_text} — {author}\\n\")  # Save as: “Quote” — Author\n",
    "\n",
    "print(\"Quotes with authors saved to quotes.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa95164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
